{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672212f6-35a1-4a19-b4a7-71b8fc948fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in ./venv/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# This command tells pip to install a version of numpy that is less than 2.0\n",
    "# Using a specific stable version like 1.26.4 is often the most reliable approach.\n",
    "!{sys.executable} -m pip install \"numpy==1.26.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ef1d3c-ea34-461d-9abe-52a50fef264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and paths defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryadoshii/Desktop/stock-return-prediction/venv/lib/python3.12/site-packages/pandas_ta/__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import os\n",
    "from tqdm import tqdm # A library to show progress bars\n",
    "\n",
    "# ... and the rest of the code in that cell ...\n",
    "\n",
    "# Define file paths\n",
    "RAW_DATA_DIR = \"../data/raw\"\n",
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "\n",
    "# Create the processed data directory if it doesn't exist\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "print(\"Libraries imported and paths defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34988a9-2bff-4fce-8810-9c3a86f03ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102 data files. Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 102/102 [00:00<00:00, 771.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 102 tickers.\n",
      "Sample data for BA:\n",
      "                  Open        High         Low       Close    Volume\n",
      "Date                                                                \n",
      "2021-01-04  202.720001  210.199997  202.490005  210.000000  21225600\n",
      "2021-01-05  211.630005  213.350006  204.600006  204.740005  19338300\n",
      "2021-01-06  211.029999  215.610001  209.339996  210.220001  16202200\n",
      "2021-01-07  212.710007  216.600006  211.779999  213.389999  14474100\n",
      "2021-01-08  209.899994  214.100006  208.160004  213.610001  14144000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to hold the data for each ticker\n",
    "stock_data = {}\n",
    "\n",
    "# Get a list of all CSV files in the raw data directory\n",
    "csv_files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.csv')]\n",
    "\n",
    "print(f\"Found {len(csv_files)} data files. Loading...\")\n",
    "\n",
    "# Define the expected columns after skipping rows\n",
    "# 'Date' will be the index, so it's not a regular column\n",
    "expected_columns = ['Open', 'High', 'Low', 'Close', 'Volume'] # Adjusted and original columns\n",
    "\n",
    "for file in tqdm(csv_files):\n",
    "    ticker = file.split('.')[0]\n",
    "    file_path = os.path.join(RAW_DATA_DIR, file)\n",
    "    \n",
    "    try:\n",
    "        # --- CRITICAL CHANGE HERE ---\n",
    "        # We tell pandas to skip the first 3 rows (0-indexed means rows 0, 1, 2)\n",
    "        # Then we manually provide the column names, since the file's headers are messy\n",
    "        df = pd.read_csv(\n",
    "            file_path, \n",
    "            header=None,         # No header row in the file we're using\n",
    "            skiprows=3,          # Skip the first 3 messy header rows\n",
    "            names=['Date', 'Open', 'High', 'Low', 'Close', 'Volume'], # Explicitly set column names\n",
    "            index_col='Date',    # Set 'Date' as the index\n",
    "            parse_dates=True     # Parse the 'Date' column as datetime objects\n",
    "        )\n",
    "        \n",
    "        # Some yfinance downloads might have duplicate columns if auto_adjust=True\n",
    "        # removes Adjusted Close, but the old files included it.\n",
    "        # Let's ensure we only have the columns we expect\n",
    "        df = df[expected_columns]\n",
    "\n",
    "        # If the file is empty or became empty after cleaning, skip it\n",
    "        if df.empty:\n",
    "            print(f\"\\n⚠️ Warning: File '{file}' is empty or became empty after cleaning. Skipping.\")\n",
    "            continue # Go to the next file in the loop\n",
    "            \n",
    "        stock_data[ticker] = df\n",
    "\n",
    "    except Exception as e: # Catch any type of error during reading\n",
    "        print(f\"\\n❌ Error processing file: {file}\")\n",
    "        print(f\"   The specific error was: {e}\")\n",
    "        print(\"   This file might be malformed or has unexpected content. Skipping.\")\n",
    "        continue # Go to the next file in the loop\n",
    "\n",
    "\n",
    "# Now, let's check if we have any data and inspect it\n",
    "if stock_data:\n",
    "    # Get the first available ticker to show a sample.\n",
    "    sample_ticker = list(stock_data.keys())[0]\n",
    "    print(f\"\\nSuccessfully loaded {len(stock_data)} tickers.\")\n",
    "    print(f\"Sample data for {sample_ticker}:\")\n",
    "    print(stock_data[sample_ticker].head())\n",
    "else:\n",
    "    print(\"\\nNo data was loaded successfully. Please check the files in data/raw.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c43082-55a3-4c77-bc62-7a9a708a4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price,Close,High,Low,Open,Volume\n",
      "Ticker,AAPL,AAPL,AAPL,AAPL,AAPL\n",
      "Date,,,,,\n",
      "2021-01-04,126.23969268798828,130.33679730921182,123.6546114053764,130.24900571878715,143301900\n",
      "2021-01-05,127.80046844482422,128.5125953811988,125.28367234969814,125.73240966734548,97664900\n"
     ]
    }
   ],
   "source": [
    "# The '!' lets us run a terminal command. 'head -n 5' shows the top 5 lines of a file.\n",
    "!head -n 5 ../data/raw/AAPL.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4196b4-298c-40fc-b03f-a38e67ec004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering function created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_features(df, spy_df, vix_df):\n",
    "    data = df.copy()\n",
    "    data.ta.rsi(length=14, append=True)\n",
    "    data.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "    data.ta.bbands(length=20, std=2, append=True)\n",
    "    data.ta.atr(length=14, append=True)\n",
    "    data.ta.obv(append=True)\n",
    "    data.ta.sma(length=20, append=True)\n",
    "    data.ta.sma(length=50, append=True)\n",
    "    stoch = ta.stoch(high=data['High'], low=data['Low'], close=data['Close'], k=14, d=3, smooth_k=3)\n",
    "    data = data.join(stoch)\n",
    "    \n",
    "    data['return_5d'] = data['Close'].pct_change(5)\n",
    "    data['return_10d'] = data['Close'].pct_change(10)\n",
    "    data['volume_momentum'] = data['Volume'] / data['Volume'].rolling(20).mean()\n",
    "    data['bollinger_pos'] = (data['Close'] - data['BBL_20_2.0']) / (data['BBU_20_2.0'] - data['BBL_20_2.0'])\n",
    "    data['z_score_20d'] = (data['Close'] - data['SMA_20']) / data['Close'].rolling(20).std()\n",
    "    data['volatility_20d'] = data['Close'].pct_change().rolling(20).std() * (252**0.5)\n",
    "    \n",
    "    spy_returns = spy_df['Close'].pct_change()\n",
    "    data['relative_volatility'] = data['volatility_20d'] / (spy_returns.rolling(20).std() * (252**0.5))\n",
    "    data['beta_20d'] = data['Close'].pct_change().rolling(20).corr(spy_returns)\n",
    "    data['vix_level'] = vix_df['Close']\n",
    "    \n",
    "    for lag in [1, 3, 5]:\n",
    "        data[f'rsi_lag_{lag}'] = data['RSI_14'].shift(lag)\n",
    "    \n",
    "    data['target_5d_forward_return'] = data['Close'].pct_change(5).shift(-5)\n",
    "    return data\n",
    "\n",
    "print(\"Feature engineering function created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7507f202-cb79-4b1e-953c-0987e7ac9524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying feature engineering to all stocks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 167.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature engineering complete.\n",
      "Final dataset shape: (95000, 34)\n"
     ]
    }
   ],
   "source": [
    "spy_df = stock_data.pop('SPY')\n",
    "vix_df = stock_data.pop('^VIX')\n",
    "all_features = []\n",
    "print(\"Applying feature engineering to all stocks...\")\n",
    "\n",
    "for ticker, df in tqdm(stock_data.items()):\n",
    "    features = create_features(df, spy_df, vix_df)\n",
    "    features['ticker'] = ticker\n",
    "    all_features.append(features)\n",
    "\n",
    "combined_df = pd.concat(all_features)\n",
    "final_df = combined_df.dropna()\n",
    "\n",
    "print(\"\\nFeature engineering complete.\")\n",
    "print(f\"Final dataset shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e681d89-9c24-4616-812f-dd11c49b3a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIG FILE LOADED ---\n",
      "Project Root is: /Users/aryadoshii/Desktop/stock-return-prediction\n",
      "Processed file will be at: /Users/aryadoshii/Desktop/stock-return-prediction/data/processed/all_features.parquet\n",
      "--------------------------\n",
      "✅ Attempted to save data to:\n",
      "/Users/aryadoshii/Desktop/stock-return-prediction/data/processed/all_features.parquet\n",
      "\n",
      "✅ SUCCESS: File has been verified and exists at the correct path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "# Use the absolute path from the config file for saving\n",
    "output_path = config.PROCESSED_FILE\n",
    "final_df.to_parquet(output_path)\n",
    "\n",
    "print(f\"✅ Attempted to save data to:\\n{output_path}\")\n",
    "\n",
    "# --- THE MOST IMPORTANT VERIFICATION STEP ---\n",
    "if os.path.exists(output_path):\n",
    "    print(\"\\n✅ SUCCESS: File has been verified and exists at the correct path.\")\n",
    "else:\n",
    "    print(\"\\n❌ CRITICAL FAILURE: File was NOT found after saving. Check permissions or disk space.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8535f9f0-839b-4940-9691-c6f06fb26b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aryadoshii/Desktop/stock-return-prediction\n",
      "Libraries imported and paths defined from config.py.\n",
      "Processed data will be saved to: /Users/aryadoshii/Desktop/stock-return-prediction/data/processed/all_features.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "# Since config.py is in the project root, we add that to the path\n",
    "sys.path.append('/Users/aryadoshii/Desktop/stock-return-prediction')\n",
    "import config\n",
    "\n",
    "# Use the absolute paths from the config file\n",
    "RAW_DATA_DIR = config.RAW_DATA_DIR\n",
    "PROCESSED_DATA_DIR = config.PROCESSED_DATA_DIR\n",
    "\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "print(\"Libraries imported and paths defined from config.py.\")\n",
    "print(f\"Processed data will be saved to: {config.PROCESSED_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2e3d2f-0f0e-4aaa-81e9-39f32513d0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-prediction-env",
   "language": "python",
   "name": "stock-prediction-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
